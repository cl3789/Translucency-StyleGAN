{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Tw6VUfKWpvo",
    "outputId": "0354e125-e782-4ce9-aac2-ec30b26f36d2"
   },
   "outputs": [],
   "source": [
    "## Use the latest StyleGAN github\n",
    "!git clone https://github.com/NVlabs/stylegan3.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tyWd2BNnW_E-",
    "outputId": "609903f3-6c98-4fb7-cd64-ac34552abef9"
   },
   "outputs": [],
   "source": [
    "#@markdown Install additional dependencies\n",
    "!pip install click requests pyspng tqdm pyopengl==3.1.5 ninja==1.10.2 imageio-ffmpeg==0.4.3 imgui==1.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jt89D45UXl6P",
    "outputId": "7d5bd6c3-d527-417e-fcd5-645fb788e697"
   },
   "outputs": [],
   "source": [
    "#@markdown Move working directory\n",
    "\n",
    "## move to the stylegan3 folder\n",
    "%cd /content/stylegan3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rUxG3x_WO9C",
    "outputId": "cf7537de-8019-479e-8727-77ce74136390"
   },
   "outputs": [],
   "source": [
    "#@markdown Import libraries\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "from typing import List, Optional\n",
    "\n",
    "import legacy\n",
    "import click\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "from sklearn.decomposition import FastICA, PCA, IncrementalPCA, MiniBatchSparsePCA, SparsePCA, KernelPCA\n",
    "\n",
    "import dnnlib\n",
    "\n",
    "print(torch.version.cuda)\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.get_device_name(device=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OHOzgfipZLyo"
   },
   "outputs": [],
   "source": [
    "#@markdown Set up random seed for reproducibility\n",
    "\n",
    "random.seed(30)\n",
    "sample_size = 20\n",
    "seed_list = random.sample(range(0, 100000), sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "cFOnQt2DWoUw"
   },
   "outputs": [],
   "source": [
    "#@markdown Utility functions\n",
    "def loading_network(network_pkl):\n",
    "  \"\"\"\n",
    "  Load pretrained StyleGAN network.\n",
    "  network_pkl: the path to the pretrained network .pkl file.\n",
    "  e.g., \"network-snapshot-soap8k-day9-403-pytorch.pkl\"\n",
    "  \"\"\"\n",
    "  print('Loading networks from \"%s\"...' % network_pkl)\n",
    "  device = torch.device('cuda')\n",
    "  with dnnlib.util.open_url(network_pkl) as f:\n",
    "      G = legacy.load_network_pkl(f)['G_ema'].to(device) \n",
    "\n",
    "  print(\"Loading finished\")\n",
    "  \n",
    "  return G\n",
    "\n",
    "\n",
    "def edit_layer(w_codes, direction, step, layer_indices, start_distance, end_distance):  \n",
    "  \"\"\"\n",
    "  Edit a subset of latent code.\n",
    "  w_codes: a list of W or W+ latent codes.\n",
    "  direction: the vector direction to move along in the latent space.\n",
    "  step: the number of intermediate steps.\n",
    "  layer_indices: a list of layer index. e.g., [6,7,8]\n",
    "  start_distance: distance from the starting latent code\n",
    "  end_distance: distance from the end latent code\n",
    "\n",
    "  \"\"\"\n",
    "  x = w_codes[:, np.newaxis]\n",
    "\n",
    "  results = np.tile(x, [step if axis == 1 else 1 for axis in range(x.ndim)])\n",
    "  \n",
    "  is_manipulatable = np.zeros(results.shape, dtype=bool)\n",
    "\n",
    "  distance = np.linspace(start_distance, end_distance, step)\n",
    "  l = distance.reshape(\n",
    "      [step if axis == 1 else 1 for axis in range(x.ndim)])\n",
    "  \n",
    "  is_manipulatable[:, :, layer_indices] = True\n",
    "  results = np.where(is_manipulatable,  x + l * direction, results)\n",
    "\n",
    "  return results\n",
    "\n",
    "def generate_from_w(G, w_codes, noise_mode = \"const\"):\n",
    "  \"\"\"\n",
    "  Generate a list of images from latent codes (either W or W+)\n",
    "  G: the loaded StyleGAN generator\n",
    "  w_codes: a list of latent codes (either W or W+)\n",
    "  \n",
    "  \"\"\"\n",
    "  device = torch.device('cuda')\n",
    "  w_codes = torch.tensor(w_codes, device=device) # pylint: disable=not-callable\n",
    "  assert w_codes.shape[1:] == (G.num_ws, G.w_dim)\n",
    "\n",
    "  generated = []\n",
    "  for idx, w in enumerate(w_codes):\n",
    "      img = G.synthesis(w.unsqueeze(0), noise_mode=noise_mode)\n",
    "      img = (img.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
    "      img = PIL.Image.fromarray(img[0].cpu().numpy(), 'RGB')\n",
    "      generated.append(img)\n",
    "  return generated \n",
    "\n",
    "def plot_subplot(img_list, n_row, n_col, width, height, show_label=True):\n",
    "    \"\"\"\n",
    "    Create (x_row, n_col) subplot of a list of images.\n",
    "    img_list: a list of PIL images\n",
    "    n_row: number of rows\n",
    "    n_col: number of columns\n",
    "    width: width of subplot\n",
    "    height: height of subplot\n",
    "    show_label: if True, the column and row indices are shown\n",
    "\n",
    "    \"\"\"\n",
    "    px = 1/plt.rcParams['figure.dpi']  # pixel in inches\n",
    "    fig, axes = plt.subplots(nrows=n_row, ncols=n_col, figsize=(width, height))\n",
    "    plt.subplots_adjust(hspace = .001)\n",
    "\n",
    "    count = -1 \n",
    "\n",
    "    if n_row==1:\n",
    "      for j in range(n_col):\n",
    "        count = count+1\n",
    "        axes[j].imshow(img_list[count])\n",
    "        axes[j].grid(False)\n",
    "        axes[j].set_xticks([])\n",
    "        axes[j].set_yticks([])\n",
    "\n",
    "    else:\n",
    "      for i in range(n_row):\n",
    "          for j in range(n_col):\n",
    "              count = count+1\n",
    "              axes[i,j].imshow(img_list[count]) ## show image\n",
    "              axes[i,j].grid(False)\n",
    "              axes[i,j].set_xticks([])\n",
    "              axes[i,j].set_yticks([])\n",
    "            \n",
    "    rows = ['Row label {}'.format(row) for row in range(n_row)]    ## labels on the Rows\n",
    "    cols = ['Column label {}'.format(col) for col in range(n_col)] ## labels on the Columns\n",
    "\n",
    "    if show_label:\n",
    "        rows = ['Row label {}'.format(row) for row in range(n_row)]    ## labels on the Rows\n",
    "        cols = ['Column label {}'.format(col) for col in range(n_col)] ## labels on the Columns\n",
    "        for ax, col in zip(axes[0], cols):\n",
    "            ax.set_title(col, size='small')\n",
    "\n",
    "        for ax, row in zip(axes[:,0], rows):\n",
    "            ax.set_ylabel(row, rotation=90, size='small')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show() \n",
    "\n",
    "def convert_rgb(images, scale_contrast):\n",
    "  \"\"\"Convert the image to the pixel range for visualization\"\"\"  \n",
    "  images = np.transpose(images, [0, 2, 3, 1])\n",
    "  images = images - np.min(images)\n",
    "  images = images / np.max(images)\n",
    "  images = scale_contrast*128*(images - np.mean(images)) + 128\n",
    "  images[np.where(images<0)] = 0\n",
    "  images[np.where(images>255)] = 255\n",
    "  images = images.astype('uint8')\n",
    "  return images\n",
    "  \n",
    "def convert_images_to_uint8(images, drange=[-1,1]):\n",
    "    \"\"\"Convert a minibatch of images from float32 to uint8 with configurable dynamic range.\n",
    "    Can be used as an output transformation for Network.run().\n",
    "    \"\"\"\n",
    "    images = images.permute(0, 2, 3, 1) \n",
    "    scale = 255 / (drange[1] - drange[0])\n",
    "    images = images * scale + (0.5 - drange[0] * scale)\n",
    "    return images.clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "\n",
    "def get_intermediate_trgb(G, w_codes):\n",
    "  \"\"\"Collect the trgb layers' outputs\"\"\"\n",
    "  G_1 = copy.deepcopy(G)\n",
    "  w_codes = torch.tensor(w_codes, device=\"cuda:0\")\n",
    "  \n",
    "  activation_layer = {}\n",
    "  def get_activation_layer(name):\n",
    "      def hook(model, input, output):\n",
    "          activation_layer[name] = output.detach()\n",
    "      return hook\n",
    "\n",
    "  for name, module in G_1.synthesis.named_modules():\n",
    "    if \"torgb\" in name and \"affine\" not in name:\n",
    "      print(name)\n",
    "      module.register_forward_hook(get_activation_layer(name))\n",
    "  img = G_1.synthesis(w_codes,  noise_mode=\"const\")\n",
    "  activation_layer[\"b1024.torgb\"] = img\n",
    "  return activation_layer\n",
    "\n",
    "def plot_generative_stages(out_levels):\n",
    "  \"\"\"Plot intemediate generative step at each major resolution\"\"\"\n",
    "  intermediate_trgb = []\n",
    "  list_layers = list(out_levels.keys())[1:]\n",
    "  for layer in list_layers:\n",
    "    layer = out_levels[layer].to(torch.float32)\n",
    "    trgb_img = convert_rgb(layer.cpu().numpy(), 2)\n",
    "    img = PIL.Image.fromarray(trgb_img[1], 'RGB')\n",
    "    # trgb_img = convert_images_to_uint8(layer)\n",
    "    # img = PIL.Image.fromarray(trgb_img[0].cpu().numpy(), 'RGB')\n",
    "\n",
    "    intermediate_trgb.append(img)\n",
    "\n",
    "  levels = [\"8 x 8\", \"16 x 16\", \"32 x 32\", \"64 x 64\", \"128 x 128\", \"256 x 256\", \"512 x 512\", \"1024 x 1024\"]\n",
    "  fig, axs = plt.subplots(1, len(intermediate_trgb), figsize=(20, 10))\n",
    "  for i, ax, image in zip(levels, axs, intermediate_trgb):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(str(i))\n",
    "    ax.grid(False)\n",
    "    ax.axis('off')\n",
    "  plt.show()\n",
    "\n",
    "  return intermediate_trgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiV386k8Y2Aj",
    "outputId": "1342f0d4-8597-451a-e517-58232d326871"
   },
   "outputs": [],
   "source": [
    "## Please change the path of the model (.pkl file) accordingly\n",
    "G_soap_path = \"/content/drive/MyDrive/StyleGAN2-multidomain/soap/training-resume-soap8k-day9-403pkl/00000-stylegan2-soap_8k_1024res-gpus1-batch4-gamma10/network-snapshot-soap8k-day9-403-pytorch.pkl\"\n",
    "G_soap = loading_network(G_soap_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Gbhy0eHld5St"
   },
   "outputs": [],
   "source": [
    "#@markdown Visualization of Latent space\n",
    "\n",
    "class visualize_latent_space():\n",
    "  def __init__(self, \n",
    "               G,\n",
    "               ):\n",
    "    self.G = copy.deepcopy(G) \n",
    "\n",
    "    \n",
    "  def sample_W_codes(\n",
    "    self,\n",
    "    seeds: Optional[List[int]],\n",
    "    truncation_psi: float,\n",
    "    noise_mode: str,\n",
    "    class_idx: Optional[int],\n",
    "    ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Sample random latent code from W latent space\n",
    "    G: the loaded StyleGAN generator.\n",
    "    seeds: a list of seeds.\n",
    "    save_img: if False, only the W latent codes are returned as a list.\n",
    "              if True, the corresponding generated images are also returned as a list\n",
    "    truncation_psi: the diversity of the generated images.\n",
    "    noise_mode: use \"const\" for constant noise.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    if seeds is None:\n",
    "        print('--seeds option is required when not using --projected-w')\n",
    "\n",
    "    label = torch.zeros([1, self.G.c_dim], device=device)\n",
    "    if self.G.c_dim != 0:\n",
    "      if class_idx is None:\n",
    "        print('Must specify class label with --class when using a conditional network')\n",
    "      label[:, class_idx] = 1\n",
    "    else:\n",
    "      if class_idx is not None:\n",
    "        print('--seeds option is required when not using --projected-w')\n",
    "\n",
    "    w_samples = []\n",
    "\n",
    "    # Sample W latent codes.\n",
    "    for seed_idx, seed in enumerate(seeds):\n",
    "      print('Generating image for seed %d (%d/%d) ...' % (seed, seed_idx, len(seeds)))\n",
    "      z = torch.from_numpy(np.random.RandomState(seed).randn(1, self.G.z_dim)).to(device)\n",
    "      w_sample = self.G.mapping(z, None)[0]\n",
    "      w_samples.append(w_sample)\n",
    "\n",
    "    return torch.stack(w_samples).cpu().numpy()\n",
    "\n",
    "  def morph_layers(\n",
    "    self,\n",
    "    w_codes,\n",
    "    steps\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Morph the specified layers in the latent space for \n",
    "    a pair of source and target latent codes \n",
    "    w_codes: a list of latent codes (either W or W+). If None, \"W\" space is used\n",
    "    steps: number of intermediate steps between source and target\n",
    "    \"\"\"\n",
    "\n",
    "    if w_codes is None:\n",
    "      print(\"sample 2 points from W space:\")\n",
    "      sample_size = 2\n",
    "      seed_list = random.sample(range(0, 100000), sample_size)    \n",
    "      w_codes = self.sample_W_codes(seeds = seed_list, truncation_psi = 0.7, noise_mode=\"const\", class_idx = None)\n",
    "    \n",
    "    # print(w_codes.shape)\n",
    "\n",
    "    w1 = w_codes[0] # source latent code\n",
    "    w2 = w_codes[1] # target latent code\n",
    "    direction = w2 - w1 ## the direction from source to target latent code\n",
    "\n",
    "    # Note:\n",
    "    # early-layers manipulation: list(range(6)): \n",
    "    # middle-layers manipulation: [6,7,8]\n",
    "    # later-layers manipulation: list(range(9,18))\n",
    "\n",
    "    for layers in [list(range(18)), list(range(6)), [6,7,8], list(range(9,18))]:\n",
    "      print(\"Changed layers index:\", layers)\n",
    "      res = edit_layer(np.expand_dims(w1, axis=0), direction = direction, step=steps, layer_indices = layers, start_distance=0, end_distance=1)\n",
    "      imgs = generate_from_w(self.G, res[0])\n",
    "      plot_subplot(imgs, n_row=1, n_col=steps, width=30, height=15, show_label=False) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUEmQFaOgDuL"
   },
   "outputs": [],
   "source": [
    "vis_soap = visualize_latent_space(G_soap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7kOm9yxMgNWR"
   },
   "outputs": [],
   "source": [
    "#@markdown Visualize the effect of layer-manipulations in W space:\n",
    "vis_soap.morph_layers(w_codes=None, steps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYor6Cu7xI6L"
   },
   "source": [
    "# Encode real images into W+ latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAbf-aDTxPNp",
    "outputId": "73896110-69fd-45fe-bf28-00da3887d2b8"
   },
   "outputs": [],
   "source": [
    "## Move to the directory of the pixel2style2pixel encoder\n",
    "%cd /content/pixel2style2pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0_2m9930gbQe",
    "outputId": "50114302-2b74-4e47-d6a8-b67739758f4e"
   },
   "outputs": [],
   "source": [
    "# !python scripts/inference.py \\\n",
    "# --exp_dir=/content/drive/MyDrive/Real_Fake_soaps_pavlovia/milk_glycerin_soap1000/milky_soap_encoded \\\n",
    "# --checkpoint_path=/content/drive/MyDrive/pixel2style2pixel/soap_encoded_models/soap_encoder_8k_1024res_pkl09_403in_Wplus/checkpoints/best_model.pt \\\n",
    "# --data_path=/content/drive/MyDrive/Real_Fake_soaps_pavlovia/milk_glycerin_soap1000/milky_soap \\\n",
    "# --test_batch_size=5 \\\n",
    "# --test_workers=4 \\\n",
    "# --save_latent_type=Wplus_all \\\n",
    "# --couple_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-j1n54-6x6E4"
   },
   "outputs": [],
   "source": [
    "#@markdown Load W+ latent codes of milky and glycerin soaps\n",
    "# Please change the path to your save .npy of the encoded images.\n",
    "opaque_samples = np.load(\"/content/drive/MyDrive/Real_Fake_soaps_pavlovia/milk_glycerin_soap1000/milky_soap_encoded/Wplus_all_layers.npy\")\n",
    "trans_samples = np.load(\"/content/drive/MyDrive/Real_Fake_soaps_pavlovia/milk_glycerin_soap1000/glycerin_soap_encoded/Wplus_all_layers.npy\")\n",
    "w_plus = np.vstack((opaque_samples,trans_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 837
    },
    "id": "Yw7eyWGFyPBp",
    "outputId": "43c3a731-4b2e-4a99-baa6-5aee15822fe8"
   },
   "outputs": [],
   "source": [
    "#@markdown Visualize the effect of layer-manipulations in W+ space:\n",
    "opaque_index = random.randrange(500)\n",
    "trans_index = random.randrange(500,1000)\n",
    "print(opaque_index, trans_index)\n",
    "w_codes_selected = w_plus[[opaque_index,trans_index]] ## select 1 opaque and 1 glycerin soap\n",
    "vis_soap.morph_layers(w_codes=w_codes_selected, steps=5)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
