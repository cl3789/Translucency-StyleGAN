{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f586c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import fnmatch, re\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score,pairwise_distances\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1d23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_rate(df):\n",
    "    conf_mat = confusion_matrix(df[\"ground_truth\"], df[\"Judgement.keys\"], labels=[\"real\", \"fake\"])\n",
    "    row,_ = df.shape\n",
    "    num_category = row/2\n",
    "    print(row)\n",
    "    real_err_rate = conf_mat[0,1]/num_category\n",
    "    fake_err_rate = conf_mat[1,0]/num_category\n",
    "    print(conf_mat)\n",
    "    print(\"Error rate for real images:\", real_err_rate)\n",
    "    print(\"Error rate for fake images:\", fake_err_rate)\n",
    "    \n",
    "    accuracy = accuracy_score(df[\"ground_truth\"], df[\"Judgement.keys\"])\n",
    "    \n",
    "    return accuracy, real_err_rate, fake_err_rate\n",
    "\n",
    "def drop_demos(data):\n",
    "    data = data.drop(range(0,3))\n",
    "    data['ground_truth'] = data['ground_truth'].astype(int)\n",
    "    data = data.replace(0,\"fake\")\n",
    "    data = data.replace(1,\"real\")\n",
    "    return data\n",
    "\n",
    "def process_individual(file_path, file_name):\n",
    "    participant_file_path = os.path.join(file_path, file_name)\n",
    "    participant_data = pd.read_csv(participant_file_path)\n",
    "    participant_data = drop_demos(participant_data)\n",
    "    participant_data[\"result\"] = participant_data.apply(lambda x: \"correct\" if x['ground_truth'] == x['Judgement.keys'] else \"wrong\", axis=1)\n",
    "    data = participant_data[[\"Trial_num\",\"image_name\",\"image_path\",\"ground_truth\",\"Judgement.keys\",\"result\"]]\n",
    "    rep1 = data.iloc[0:300]\n",
    "    rep2 = data.iloc[300:]\n",
    "    \n",
    "    select_fake_rep1,_ = rep1[rep1[\"Judgement.keys\"] == \"fake\"].shape\n",
    "    #print(select_fake_rep1)\n",
    " \n",
    "    select_fake_rep2,_ = rep2[rep2[\"Judgement.keys\"] == \"fake\"].shape\n",
    "    #print(select_fake_rep2)\n",
    "    \n",
    "    \n",
    "    print(\"Repetition 1:\")\n",
    "    accuracy_rep1, real_err_rate_rep1, fake_err_rate_rep1 = compute_error_rate(rep1)\n",
    "    print(\"Repetition 2:\")\n",
    "    accuracy_rep2, real_err_rate_rep2, fake_err_rate_rep2 = compute_error_rate(rep2)\n",
    "    print(\"Overall:\")\n",
    "    accuracy_all, real_err_rate_all, fake_err_rate_all = compute_error_rate(data)\n",
    "\n",
    "    \n",
    "    wrong_rep1 = rep1[rep1[\"result\"] == \"wrong\"][\"image_name\"]\n",
    "    wrong_rep2 = rep2[rep2[\"result\"] == \"wrong\"][\"image_name\"]\n",
    "\n",
    "    wrong_in_both = list(set(wrong_rep1).intersection(wrong_rep2))\n",
    "    print(\"Wrong in both:\",len(wrong_in_both))\n",
    "    \n",
    "    encoded_misjudged = list(filter(lambda x: x.startswith('encoded'), wrong_in_both))\n",
    "    \n",
    "    real_misjudged = list(set(wrong_in_both) - set(encoded_misjudged))\n",
    "    \n",
    "    print(\"Number of encoded image been misjudged in both repetition:\", len(encoded_misjudged)/150)\n",
    "    print(\"Number of real image been misjudged in both repetition:\", len(real_misjudged)/150)\n",
    "    \n",
    "    df_person = pd.DataFrame({'participant':file_name,\n",
    "                              'real_err_rate_rep1':real_err_rate_rep1,\n",
    "                              'fake_err_rate_rep1':fake_err_rate_rep1,\n",
    "                              'real_err_rate_rep2':real_err_rate_rep2,\n",
    "                              'fake_err_rate_rep2':fake_err_rate_rep2,\n",
    "                              'real_err_rate_all':real_err_rate_all,\n",
    "                              'fake_err_rate_all':fake_err_rate_all,\n",
    "                              'accuracy_rep1': accuracy_rep1,\n",
    "                              'accuracy_rep2': accuracy_rep2,\n",
    "                              'accuracy_all': accuracy_all,\n",
    "                              'select_fake_rep1':select_fake_rep1,\n",
    "                              'select_fake_rep2':select_fake_rep2,\n",
    "                              'real_misjudged_img_both_rep':[real_misjudged], \n",
    "                              'encoded_misjudged_img_both_rep':[encoded_misjudged], \n",
    "                              'num_real_misjudged_both_rep':len(real_misjudged),\n",
    "                              'num_encoded_misjudged_both_rep':len(encoded_misjudged)}) \n",
    "    \n",
    "    \n",
    "    person_result = data[[\"result\"]]\n",
    "    \n",
    "    return df_person, person_result\n",
    "\n",
    "\n",
    "def process_all_people(file_path):\n",
    "    files = list(filter(lambda f: os.path.isfile(os.path.join(file_path,f)), os.listdir(file_path)))\n",
    "\n",
    "    df_stat = pd.DataFrame()\n",
    "    \n",
    "    trial_num_path = os.path.join(file_path, files[0])\n",
    "    print(trial_num_path)\n",
    "    \n",
    "    aggregate = pd.read_csv(trial_num_path, usecols = [\"Trial_num\",\"image_name\",\"image_path\",\"ground_truth\"])\n",
    "    aggregate = drop_demos(aggregate)\n",
    "    \n",
    "    for person_data in files:\n",
    "        if fnmatch.fnmatch(person_data, '*.csv'):\n",
    "            df_person, person_result = process_individual(file_path, person_data)\n",
    "            df_stat = df_stat.append(df_person, ignore_index=True)\n",
    "            aggregate = pd.concat([aggregate, person_result], axis=1)\n",
    "            print(aggregate)\n",
    "\n",
    "    aggregate[\"all_res\"] =  aggregate[aggregate.columns[4]].values.tolist()\n",
    "    aggregate[\"num_correct\"] = aggregate['all_res'].apply(lambda x: x.count(\"correct\"))\n",
    "    aggregate[\"num_wrong\"] = aggregate['all_res'].apply(lambda x: x.count(\"wrong\"))\n",
    "    return df_stat,  aggregate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ed0d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the data\n",
    "data_path = \"data/Experiment1\"\n",
    "all_people_real_fake, aggregate_result = process_all_people(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede6d52",
   "metadata": {},
   "source": [
    "# Plot Error rate for each observer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(context='poster', font_scale=0.5)\n",
    "\n",
    "data_for_boxplot = [all_people_real_fake['real_err_rate_all'], all_people_real_fake['fake_err_rate_all']]\n",
    "fig1, ax1 = plt.subplots()\n",
    "labels = [\"Error rate for judging \\n Real images\", \"Error rate for judging \\n Generated images\"]\n",
    "\n",
    "\n",
    "ax1 = sns.swarmplot(data=data_for_boxplot,  palette=[\"#bdbdbd\", \"#2ca02c\"], size = 8)\n",
    "\n",
    "sns.despine()\n",
    "\n",
    "ax1.set_title(\"Distribution of Observers' Error Rate\")\n",
    "ax1.set_xticklabels(labels)\n",
    "ax1.set_ylabel(\"Observer's Error rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff291076",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Define color map\n",
    "\n",
    "real_fake_color_palette = dict(real=\"#bdbdbd\", fake=\"#2ca02c\")\n",
    "\n",
    "\n",
    "real_fake_hue_order = [\"real\",\"fake\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add percent wrong column\n",
    "\n",
    "aggregate_result[\"percent_wrong\"] = aggregate_result[\"num_wrong\"] / (aggregate_result[\"num_wrong\"] + aggregate_result[\"num_correct\"])\n",
    "\n",
    "aggregate_result[\"percent_correct\"] = 1 - aggregate_result[\"percent_wrong\"]\n",
    "\n",
    "rep = 300 * [\"Rep 1\"] + 300 * [\"Rep 2\"]\n",
    "\n",
    "aggregate_result[\"Repeat\"] = rep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a2a89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splits 2 repeats. Match the 2nd repeat to first\n",
    "rep1 = aggregate_result.iloc[0:300]\n",
    "rep2 = aggregate_result.iloc[300:]\n",
    "\n",
    "rep2 = rep2.set_index('image_name')\n",
    "rep2 = rep2.reindex(index=rep1['image_name'])\n",
    "rep2 = rep2.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378e267",
   "metadata": {},
   "source": [
    "# Show aggregated error for fake and real judgement (Figure 2C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31dcbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_result.groupby(\"ground_truth\").sum()/6000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a156894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep1.groupby(\"ground_truth\").sum()/3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3627be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep2.groupby(\"ground_truth\").sum()/3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27722b59",
   "metadata": {},
   "source": [
    "# Plot distribution of error (Figure 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8f44c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "real_fake_hue_order = [\"fake\", \"real\"]\n",
    "\n",
    "real_fake_color_palette = dict(real=\"#bdbdbd\", fake=\"#2ca02c\")\n",
    "\n",
    "\n",
    "plot_error_image = sns.displot(aggregate_result, x=\"percent_wrong\", hue=\"ground_truth\", col=\"Repeat\", stat=\"percent\", alpha=0.7,\n",
    "                               binwidth=0.1, common_norm=False, fill=True, edgecolor= \"white\",linewidth=1,\n",
    "                               hue_order=real_fake_hue_order,palette=real_fake_color_palette, legend = True)\n",
    "sns.despine(offset=10, left=False, right = True)\n",
    "plot_error_image.set(xticks=np.arange(0,1,0.1), yticks=np.arange(0,50,10))\n",
    "#plot_error_image.set(xlabel=None, ylabel=None, xticklabels=[], yticklabels=[])\n",
    "plot_error_image.set(xlabel='Percentage of observers misjudging judged an image', ylabel='Percentage of images')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
